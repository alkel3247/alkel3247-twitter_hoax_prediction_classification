{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "import nltk\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import os\n",
    "print(os.getcwd())   # Prints the current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the new path here\n",
    "os.chdir('C:\\\\Users\\\\user\\\\Desktop\\\\Twitter NLP Classification Model\\\\DATASET\\\\dataset 31-05-2020')\n",
    "df = pd.read_csv('result_prepro3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# RANDOM SAMPLE\n",
    "###\n",
    "chosen_idx = np.random.choice(len(df), replace=False, size=10000)\n",
    "df_sample = df.iloc[chosen_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_unigram = df.copy() ## USING ORIGINAL DATA \n",
    "df_unigram = df_sample.copy() ## USING SAMPLE DATA\n",
    "df_unigram = df_unigram.assign(Text6 = df['Text5'])\n",
    "df_unigram = df_unigram.assign(Binary1 = 0 )\n",
    "df_unigram = df_unigram.assign(Binary2 = 0 )\n",
    "df_unigram = df_unigram.assign(tfidf = 0)\n",
    "df_unigram['Binary1'] = df_unigram['Class'].apply(lambda x: 0 if 'hoax' in x else 1)\n",
    "df_unigram['Binary2'] = df_unigram['Class'].apply(lambda x: 1 if 'hoax' in x else 0)\n",
    "df_unigram = df_unigram[['Text6','Binary1','Binary2','tfidf']]\n",
    "\n",
    "df_bigram = df_unigram.copy()\n",
    "df_trigram = df_unigram.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# IMPORTANT , PLEASE SET OUTPUT to be either 1 or 2 columns deliberately\n",
    "############################################################################\n",
    "\n",
    "##Output combine binary 1 + binary 2 [SELECT 1 COLUMN of OUTPUT]\n",
    "# output = df_unigram[['Binary1','Binary2']].to_numpy()\n",
    "\n",
    "## Output is Binary2 [SELECT 2 COLUMNs of OUTPUT]\n",
    "output = df_unigram[['Binary2']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_unigram['Text6'].copy()\n",
    "print(data.head())\n",
    "data_unigram = data.apply(lambda row: nltk.word_tokenize(str(row)))\n",
    "print(data_unigram[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "\n",
    "#You can suppress the message with this code before importing gensim:\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "import gensim\n",
    "\n",
    "import time\n",
    "import multiprocessing\n",
    "from datetime import timedelta\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Phrases\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# SET SMALL (for trial and error purpose) OR LARGE DATA\n",
    "######################################################################\n",
    "\n",
    "small_data_status = 1\n",
    "\n",
    "if small_data_status == 1 :\n",
    "    data_input = data_unigram[0:40]\n",
    "elif small_data_status == 0 :\n",
    "    data_input = data_unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_matrix_size = 3\n",
    "\n",
    "#Word2Vec\n",
    "# Create CBOW model \n",
    "model_unigram = gensim.models.Word2Vec(data_input, min_count=1, size = embedded_matrix_size, workers=multiprocessing.cpu_count()-1, window = 3 , sg =0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.syn1)\n",
    "# print(model.syn2)\n",
    "print(model_unigram.syn1neg) #negative sampling, embedding output matrix word2vec\n",
    "print(type(model_unigram.syn1neg))\n",
    "print(model_unigram.syn1neg.shape)\n",
    "print(model_unigram.syn1neg.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_transformer = Phrases(list(data_input))\n",
    "model_bigram = gensim.models.Word2Vec(bigram_transformer[list(data_input)], min_count=1,size = 3)\n",
    "print(model_bigram.syn1neg)\n",
    "\n",
    "data_bigram = data_input.apply(lambda row:bigram_transformer[row])\n",
    "print(type(data_bigram))\n",
    "print(data_bigram)\n",
    "data_word2vec_bigram = data_bigram[0:2].apply(lambda x: model_bigram[x])\n",
    "print(data_word2vec_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_sentences = data_input.apply(lambda row: u' '.join(bigram_transformer[row]))    \n",
    "trigram_transformer = Phrases(list(bigram_sentences))\n",
    "model_trigram = gensim.models.Word2Vec(trigram_transformer[list(bigram_sentences)], min_count=1,size = 3)\n",
    "print(model_trigram.syn1neg)\n",
    "\n",
    "\n",
    "\n",
    "data_trigram = bigram_sentences.apply(lambda row:trigram_transformer[row])\n",
    "print(type(data_trigram))\n",
    "print(data_trigram)\n",
    "data_word2vec_trigram = data_trigram[0:2].apply(lambda x: model_trigram[x])\n",
    "print(data_word2vec_trigram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_word2vec_unigram = data_input.apply(lambda x: model_unigram[x])\n",
    "data_word2vec_bigram = data_bigram.apply(lambda x: model_bigram[x])\n",
    "data_word2vec_trigram = data_trigram.apply(lambda x: model_trigram[x])\n",
    "\n",
    "print(data_word2vec_unigram)\n",
    "data_word2vec_unigram2 = np.array(data_word2vec_unigram)\n",
    "print(data_word2vec_unigram2)\n",
    "\n",
    "\n",
    "print(data_word2vec_bigram)\n",
    "data_word2vec_bigram2 = np.array(data_word2vec_bigram)\n",
    "print(data_word2vec_unigram2)\n",
    "\n",
    "print(data_word2vec_trigram)\n",
    "data_word2vec_trigram2 = np.array(data_word2vec_trigram)\n",
    "print(data_word2vec_trigram2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sum all vectors\n",
    "\n",
    "data_word2vec_unigram3 = np.zeros(data_word2vec_unigram2.shape)\n",
    "data_word2vec_unigram3 = list(data_word2vec_unigram3)\n",
    "print(type(data_word2vec_unigram3))\n",
    "\n",
    "i = 0\n",
    "for x in data_word2vec_unigram2 :\n",
    "    data_word2vec_unigram3[i] = np.add.reduce(x)\n",
    "#     print(x)\n",
    "    i = i + 1\n",
    "\n",
    "    \n",
    "######################################## Bigram    \n",
    "data_word2vec_bigram3 = np.zeros(data_word2vec_bigram2.shape)\n",
    "data_word2vec_bigram3 = list(data_word2vec_bigram3)\n",
    "print(type(data_word2vec_bigram3))\n",
    "\n",
    "i = 0\n",
    "for x in data_word2vec_bigram2 :\n",
    "    data_word2vec_bigram3[i] = np.add.reduce(x)\n",
    "#     print(x)\n",
    "    i = i + 1 \n",
    "    \n",
    "################################################## Trigram    \n",
    "data_word2vec_trigram3 = np.zeros(data_word2vec_trigram2.shape)\n",
    "data_word2vec_trigram3 = list(data_word2vec_trigram3)\n",
    "print(type(data_word2vec_trigram3))\n",
    "\n",
    "i = 0\n",
    "for x in data_word2vec_trigram2 :\n",
    "    data_word2vec_trigram3[i] = np.add.reduce(x)\n",
    "#     print(x)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_word2vec_unigram3)\n",
    "print(data_word2vec_unigram3[0])\n",
    "print(len(data_word2vec_unigram3))\n",
    "data_word2vec_unigram4 = np.array(data_word2vec_unigram3)\n",
    "print(data_word2vec_unigram4)\n",
    "\n",
    "\n",
    "############### Bigram\n",
    "print(data_word2vec_bigram3)\n",
    "print(data_word2vec_bigram3[0])\n",
    "print(len(data_word2vec_bigram3))\n",
    "data_word2vec_bigram4 = np.array(data_word2vec_bigram3)\n",
    "print(data_word2vec_bigram4)\n",
    "\n",
    "################ Trigram\n",
    "print(data_word2vec_trigram3)\n",
    "print(data_word2vec_trigram3[0])\n",
    "print(len(data_word2vec_trigram3))\n",
    "data_word2vec_trigram4 = np.array(data_word2vec_trigram3)\n",
    "print(data_word2vec_trigram4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_word2vec_unigram4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Variable Input , output and number of neurons for ANN\n",
    "\n",
    "row_train_end = round(data_word2vec_unigram4.shape[0] * 0.8)\n",
    "row_test_end = data_word2vec_unigram4.shape[0]\n",
    "\n",
    "input_word2vec_unigram_training = data_word2vec_unigram4[0:row_train_end]\n",
    "input_word2vec_unigram_testing = data_word2vec_unigram4[row_train_end:row_test_end]\n",
    "output_word2vec_unigram_training = output[0:row_train_end]            #80%\n",
    "output_word2vec_unigram_testing = output[row_train_end:row_test_end]  #20%\n",
    "print(input_word2vec_unigram_training.shape)\n",
    "\n",
    "##################################### Bigram\n",
    "input_word2vec_bigram_training = data_word2vec_bigram4[0:row_train_end]\n",
    "input_word2vec_bigram_testing = data_word2vec_bigram4[row_train_end:row_test_end]\n",
    "output_word2vec_bigram_training = output[0:row_train_end]               #80%\n",
    "output_word2vec_bigram_testing = output[row_train_end:row_test_end]     #20%\n",
    "\n",
    "##################################### Trigram\n",
    "input_word2vec_trigram_training = data_word2vec_trigram4[0:row_train_end]\n",
    "input_word2vec_trigram_testing = data_word2vec_trigram4[row_train_end:row_test_end]\n",
    "output_word2vec_trigram_training = output[0:row_train_end]                #80%\n",
    "output_word2vec_trigram_testing = output[row_train_end:row_test_end]      #20%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# SET UNI-GRAM, BI-GRAM, TRI-GRAM IN THIS PART\n",
    "###############################################################################\n",
    "\n",
    "# n_grams = 1 # UNI-GRAM\n",
    "n_grams = 2 # BI-GRAM\n",
    "# n_grams = 3 # TRI-GRAM\n",
    "\n",
    "if n_grams == 1:\n",
    "    input_training_ANN = input_word2vec_unigram_training \n",
    "    input_testing_ANN = input_word2vec_unigram_testing\n",
    "    output_training_ANN = output_word2vec_unigram_training\n",
    "    output_testing_ANN = output_word2vec_unigram_testing\n",
    "    print(\"Unigram Selected \")\n",
    "    \n",
    "elif n_grams == 2:\n",
    "    input_training_ANN = input_word2vec_bigram_training\n",
    "    input_testing_ANN = input_word2vec_bigram_testing\n",
    "    output_training_ANN = output_word2vec_bigram_training\n",
    "    output_testing_ANN = output_word2vec_bigram_testing\n",
    "    print(\"Bigram Selected \")\n",
    "        \n",
    "elif n_grams == 3:\n",
    "    input_training_ANN = input_word2vec_trigram_training\n",
    "    input_testing_ANN = input_word2vec_trigram_testing\n",
    "    output_training_ANN = output_word2vec_trigram_training\n",
    "    output_testing_ANN = output_word2vec_trigram_testing\n",
    "    print(\"Trigram Selected \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf \n",
    "#import tensorflow_text as text \n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.python.ops.init_ops import glorot_normal_initializer\n",
    "from tensorflow.python.ops.init_ops import glorot_uniform_initializer\n",
    "from tensorflow.python.ops.init_ops import he_normal\n",
    "from tensorflow.python.ops.init_ops import he_uniform\n",
    "from tensorflow.python.ops.init_ops import truncated_normal_initializer\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import array \n",
    "import pandas as pd\n",
    "\n",
    "from scipy.sparse import coo_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=123)\n",
    "# BEGIN CODE TENSORFLOW \n",
    "seed_id_number = 123\n",
    "tf.random.set_seed(seed_id_number)\n",
    "def build_model():\n",
    "    model = keras.Sequential([    \n",
    "    layers.Dense(500, activation='relu' ,\n",
    "                 input_shape=[embedded_matrix_size],\n",
    "#                  kernel_initializer = truncated_normal_initializer(mean=0.0, stddev=0.05, seed=seed_id_number),\n",
    "                 kernel_initializer = he_normal(seed=seed_id_number),\n",
    "                 bias_initializer = he_normal(seed=seed_id_number)\n",
    "                ), #1st hidden layer\n",
    "    layers.Dense(350, activation='relu',\n",
    "                 kernel_initializer = he_normal(seed=seed_id_number),\n",
    "                 bias_initializer = he_normal(seed=seed_id_number)\n",
    "                ), #2nd hidden layer\n",
    "    layers.Dense(200, activation='relu',\n",
    "                 kernel_initializer = he_normal(seed=seed_id_number),\n",
    "                 bias_initializer = he_normal(seed=seed_id_number)\n",
    "                ), #3rd hidden layer\n",
    "    layers.Dense(1, activation ='sigmoid',\n",
    "                kernel_initializer = glorot_normal_initializer(seed=seed_id_number),\n",
    "                bias_initializer = glorot_normal_initializer(seed=seed_id_number)\n",
    "                ), #output layer\n",
    "    layers.Dropout(0.5)\n",
    "    ])\n",
    "\n",
    "  # (Learning Rate, Momentum,nesterov)\n",
    "    optimizer = tf.keras.optimizers.SGD(0.01,0.9,False) \n",
    "\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mean_absolute_error','accuracy'])\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create ANN Model\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display training progress by printing a single dot for each completed epoch\n",
    "class PrintDot(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if epoch % 100 == 0: print('')\n",
    "        print('.', end='')\n",
    "##Set training Iterations\n",
    "# EPOCHS = 10\n",
    "EPOCHS = 200\n",
    "#Run Training Model\n",
    "history = model.fit(\n",
    "    input_training_ANN,output_training_ANN,\n",
    "    epochs=EPOCHS, validation_split = 0.2, verbose=1,\n",
    "    callbacks=[PrintDot()]\n",
    ")\n",
    "\n",
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "hist.tail() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ##Print Weight and Bias for each layers\n",
    "weight_1, bias_1 = model.layers[0].get_weights()\n",
    "weight_2, bias_2 = model.layers[1].get_weights() \n",
    "weight_3, bias_3 = model.layers[2].get_weights()\n",
    "weight_4, bias_4 = model.layers[3].get_weights()\n",
    "# weight_5, bias_5 = model.layers[4].get_weights()\n",
    "\n",
    "print('1st layer weight :\\n',weight_1)\n",
    "print('1st layer bias : \\n',bias_1)\n",
    "\n",
    "print('2nd layer weight : \\n',weight_2)\n",
    "print('2nd layer bias : \\n',bias_2)\n",
    "\n",
    "print('3rd layer weight : \\n',weight_3)\n",
    "print('3rd layer bias : \\n',bias_3)\n",
    "\n",
    "print('4rd layer weight : \\n',weight_4)\n",
    "print('4rd layer bias : \\n',bias_4)\n",
    "\n",
    "# print('4rd layer weight : \\n',weight_5)\n",
    "# print('4rd layer bias : \\n',bias_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Plot 'Module'\n",
    "def plot_history(history):\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "    \n",
    "    ## Plot Target Output Graph\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle(' [TARGET OUTPUT TESTING]')\n",
    "    plt.plot(output_testing_ANN)\n",
    "    \n",
    "    ##Plot Mean Absolute Error vs Epoch Graph (test and train)\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Abs Error [MPG]')\n",
    "    plt.plot(hist['epoch'], hist['mean_absolute_error'],\n",
    "           label='Train Error')\n",
    "    plt.plot(hist['epoch'], hist['val_mean_absolute_error'],\n",
    "           label = 'Validation Error')\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "    ##Plot Mean Square Error vs Epoch graph (test and train)\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Square Error [$MPG^2$]')\n",
    "    plt.plot(hist['epoch'], hist['loss'],\n",
    "           label='Train Error')\n",
    "    plt.plot(hist['epoch'], hist['val_loss'],\n",
    "           label = 'Validation Error')\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "  \n",
    "##Calling plot 'Module'   \n",
    "plot_history(history)\n",
    "# plot_history(hist)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Testing Model\n",
    "test_predictions = model.predict(input_testing_ANN)\n",
    "\n",
    "print(len(test_predictions))\n",
    "test_predictions1 = test_predictions[0]\n",
    "print(test_predictions1)\n",
    "\n",
    "test_predictions1[test_predictions1 >= 0.5] = 1\n",
    "test_predictions1[test_predictions1 < 0.5] = 0\n",
    "\n",
    "print(output_testing_ANN)\n",
    "print(test_predictions)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.suptitle('Output Prediction')\n",
    "plt.plot(test_predictions)\n",
    "\n",
    "print(('Test Predictions = {}').format(test_predictions))\n",
    "\n",
    "test_predictions[test_predictions >= 0.5] = 1\n",
    "test_predictions[test_predictions < 0.5] = 0\n",
    "print(test_predictions)\n",
    "\n",
    "##Plot Prediction Error [MPG] graph\n",
    "error = test_predictions - output_testing_ANN\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(error, bins = 25, label = \"Binary1\")\n",
    "#plt.hist(error[:,1], bins = 25, label = \"Binary2\" )\n",
    "plt.xlabel(\"Prediction Error [MPG]\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_predictions[0])\n",
    "print(test_predictions[:,0])\n",
    "print(type(test_predictions[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(test_predictions1))\n",
    "print(type(df[40516:50644]))\n",
    "\n",
    "\n",
    "# if small_data_status == 1:\n",
    "#     df_testing = df.iloc[11:20,:].copy()\n",
    "\n",
    "# elif small_data_status == 0:\n",
    "#     df_testing = df.iloc[40516:50644,:].copy()\n",
    "df_testing = df.iloc[row_train_end:row_test_end,:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_testing.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing2 =  pd.DataFrame(test_predictions[:,0] , columns =['Test Predictions Binary1'])\n",
    "df_testing2['Test Predictions Binary2'] =  pd.DataFrame(test_predictions[:,0])\n",
    "df_testing2['Text5'] = df_testing['Text5'].to_numpy()\n",
    "print(df_testing2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_testing3 = df_testing2[df_testing2['Test Predictions Binary2']==0] # Filter Non-Hoax Prediction\n",
    "df_testing3 = df_testing2[df_testing2['Test Predictions Binary2']==1] # Filter Hoax Prediction\n",
    "print(df_testing3.head())\n",
    "print(df_testing3.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(df_testing3['Text5'])\n",
    "words = np.array(words)\n",
    "print(type(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion Matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "matrix = confusion_matrix(test_predictions.argmax(axis=1),output_testing_ANN.argmax(axis=1))\n",
    "print(matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_unigram = TfidfVectorizer(ngram_range=(1,1)) \n",
    "tfidf_bigram = TfidfVectorizer(ngram_range=(2,2))\n",
    "tfidf_trigram = TfidfVectorizer(ngram_range=(3,3))\n",
    "# input_word2vec_trigram_testing_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_unigram = tfidf_unigram.fit_transform(df_unigram['Text6'].values.astype('U'))\n",
    "vectors_bigram = tfidf_bigram.fit_transform(df_bigram['Text6'].values.astype('U'))\n",
    "vectors_trigram = tfidf_trigram.fit_transform(df_trigram['Text6'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectors_unigram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_word2vec_unigram_testing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectors_unigram_testing_small = vectors_unigram[11:20]\n",
    "# vectors_bigram_testing_small = vectors_bigram[11:20]\n",
    "# vectors_trigram_testing_small = vectors_trigram[11:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_unigram_testing = vectors_unigram[row_train_end:row_test_end]\n",
    "vectors_bigram_testing = vectors_bigram[row_train_end:row_test_end]\n",
    "vectors_trigram_testing = vectors_trigram[row_train_end:row_test_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_grams == 1:\n",
    "    vectors_testing_ANN = vectors_unigram[row_train_end:row_test_end]\n",
    "    tfidf_ANN = tfidf_unigram\n",
    "elif n_grams == 2:\n",
    "    vectors_testing_ANN = vectors_bigram[row_train_end:row_test_end]\n",
    "    tfidf_ANN = tfidf_bigram\n",
    "elif n_grams == 3:\n",
    "    vectors_testing_ANN = vectors_trigram[row_train_end:row_test_end]\n",
    "    tfidf_ANN = tfidf_trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2score = chi2(vectors_testing_ANN, output_testing_ANN)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chi2score.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wscores = list(zip(tfidf_ANN.get_feature_names(), chi2score))\n",
    "wchi2 = sorted(wscores, key=lambda x:x[1])\n",
    "print(wchi2[0:2])\n",
    "wchi2 = [row for row in wchi2 if not np.isnan(row[1])]\n",
    "print(wchi2[0:2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "wscores = list(zip(tfidf_ANN.get_feature_names(), chi2score)) # List\n",
    "wscores1 = [row for row in wscores if not np.isnan(row[1])]  # remove NAN value\n",
    "# wchi2 = sorted(wscores, key=lambda x:x[1]) #with NAN values\n",
    "wchi2 = sorted(wscores1, key=lambda x:x[1]) #without NAN values                    \n",
    "topchi2 = list(zip(*wchi2[-50:]))\n",
    "x = range(len(topchi2[1]))\n",
    "labels = topchi2[0]\n",
    "plt.barh(x,topchi2[1], align='center', alpha=0.2)\n",
    "plt.plot(topchi2[1], x, '-o', markersize=5, alpha=0.8)\n",
    "plt.yticks(x, labels)\n",
    "plt.xlabel('$\\chi^2$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def plot_cm(labels, predictions, p = 0.5):\n",
    "  cm = confusion_matrix(labels, predictions > p)\n",
    "  plt.figure(figsize=(5,5))\n",
    "  sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "  plt.title('Confusion matrix @{:.2f}'.format(p))\n",
    "  plt.ylabel('Actual label')\n",
    "  plt.xlabel('Predicted label')\n",
    "\n",
    "  print('Legitimate Transactions Detected (True Negatives): ', cm[0][0])\n",
    "  print('Legitimate Transactions Incorrectly Detected (False Positives): ', cm[0][1])\n",
    "  print('Fraudulent Transactions Missed (False Negatives): ', cm[1][0])\n",
    "  print('Fraudulent Transactions Detected (True Positives): ', cm[1][1])\n",
    "  print('Total Fraudulent Transactions: ', np.sum(cm[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_predictions)\n",
    "plot_cm(output_testing_ANN[:,0],test_predictions[:,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
