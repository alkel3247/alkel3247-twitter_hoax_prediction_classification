{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "import nltk\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import os\n",
    "print(os.getcwd())   # Prints the current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the new path here\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\user\\\\Desktop\\\\Twitter NLP Classification Model\\\\DATASET\\\\dataset 31-05-2020')\n",
    "df = pd.read_csv('result_prepro3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# RANDOM SAMPLE\n",
    "###\n",
    "chosen_idx = np.random.choice(len(df), replace=False, size=10000)\n",
    "df_sample = df.iloc[chosen_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unigram = df.copy() ## USING ORIGINAL DATA \n",
    "# df_unigram = df_sample.copy() ## USING SAMPLE DATA\n",
    "df_unigram = df_unigram.assign(Text6 = df['Text5'])\n",
    "df_unigram = df_unigram.assign(Binary1 = 0 )\n",
    "df_unigram = df_unigram.assign(Binary2 = 0 )\n",
    "df_unigram['Binary1'] = df_unigram['Class'].apply(lambda x: 0 if 'hoax' in x else 1) #non-hoax\n",
    "df_unigram['Binary2'] = df_unigram['Class'].apply(lambda x: 1 if 'hoax' in x else 0) #hoax\n",
    "df_unigram = df_unigram[['Text6','Binary1','Binary2']]\n",
    "\n",
    "df_unigram.dropna(subset=['Text6'], inplace = True) # exclude NA values in Text6\n",
    "\n",
    "df_bigram = df_unigram.copy()\n",
    "df_trigram = df_unigram.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_unigram.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_unigram = TfidfVectorizer(ngram_range=(1,1)) \n",
    "tfidf_bigram = TfidfVectorizer(ngram_range=(2,2))\n",
    "tfidf_trigram = TfidfVectorizer(ngram_range=(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_unigram = tfidf_unigram.fit_transform(df_unigram['Text6'].values.astype('U'))\n",
    "vectors_bigram = tfidf_bigram.fit_transform(df_bigram['Text6'].values.astype('U'))\n",
    "vectors_trigram = tfidf_trigram.fit_transform(df_trigram['Text6'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vectors_unigram.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectors_unigram.shape)\n",
    "print(vectors_bigram.shape)\n",
    "print(vectors_trigram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectors_unigram.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# IMPORTANT , PLEASE SET OUTPUT to be either 1 or 2 columns deliberately\n",
    "############################################################################\n",
    "\n",
    "##Output combine binary 1 + binary 2 [SELECT 1 COLUMN of OUTPUT]\n",
    "# output = df_unigram[['Binary1','Binary2']].to_numpy()\n",
    "\n",
    "## Output is Binary2 [SELECT 2 COLUMNs of OUTPUT]\n",
    "output = df_unigram[['Binary2']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_unigram.shape)\n",
    "print(df_bigram.shape)\n",
    "print(df_trigram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(output.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(output[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectors_unigram[:,0:10].shape) #[row,columns] # select all rows, while selecting columns with itterate 0-10 [10 columns]\n",
    "print(output[:,0:2]) #[row,columns] # select all rows, while selecting columns with itterate 0-2 [2 columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_unigram = tfidf_unigram.get_feature_names()\n",
    "print(len(feature_names_unigram))\n",
    "\n",
    "feature_names_bigram = tfidf_bigram.get_feature_names()\n",
    "print(len(feature_names_bigram))\n",
    "\n",
    "feature_names_trigram = tfidf_trigram.get_feature_names()\n",
    "print(len(feature_names_trigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# Displaying N-gram word results (Cant display all due to memory issue, select one & uncomment it)\n",
    "#######################################\n",
    "# print(feature_names_unigram)\n",
    "# print(feature_names_bigram)   \n",
    "# print(feature_names_trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_unigram=vectors_unigram # selecting all unigram tf-idf score (24446 scores)\n",
    "print(vectors_unigram.shape)\n",
    "\n",
    "vectors_bigram=vectors_bigram # selecting all bigram tf-idf scores (118397 scores)\n",
    "print(vectors_bigram.shape) \n",
    "\n",
    "vectors_trigram=vectors_trigram  # selecting all trigram tf-idf score (114664 scores)\n",
    "print(vectors_trigram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable Input , output and number of neurons for ANN\n",
    "\n",
    "row_train_end = round(vectors_unigram.shape[0] * 0.8)\n",
    "row_test_end = vectors_unigram.shape[0]\n",
    "\n",
    "# Unigram\n",
    "input_unigram_training = vectors_unigram[0:row_train_end]                #80%\n",
    "output_unigram_training = output[0:row_train_end]                        #80%\n",
    "input_unigram_testing = vectors_unigram [row_train_end:row_test_end]     #20%\n",
    "output_unigram_testing = output[row_train_end:row_test_end]              #20%\n",
    "print(input_unigram_training.shape)\n",
    "print(output_unigram_training.shape)\n",
    "print(input_unigram_testing.shape)\n",
    "print(output_unigram_testing.shape)\n",
    "\n",
    "# Bigram\n",
    "input_bigram_training = vectors_bigram[0:row_train_end]                 #80%\n",
    "output_bigram_training = output[0:row_train_end]                        #80%\n",
    "input_bigram_testing = vectors_bigram [row_train_end:row_test_end]      #20%\n",
    "output_bigram_testing = output[row_train_end:row_test_end]              #20%\n",
    "\n",
    "# Trigram\n",
    "input_trigram_training = vectors_trigram[0:row_train_end]                #80%\n",
    "output_trigram_training = output[0:row_train_end]                        #80%\n",
    "input_trigram_testing = vectors_trigram [row_train_end:row_test_end]     #20%\n",
    "output_trigram_testing = output[row_train_end:row_test_end]              #20%\n",
    "\n",
    "\n",
    "uni_neurons = vectors_unigram.shape[1]  # input size neuron for unigram\n",
    "bi_neurons = vectors_bigram.shape[1]    # input size neuron for bigram\n",
    "tri_neurons = vectors_trigram.shape[1]  # input size neuron for trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(uni_neurons) # input size for unigram\n",
    "print(bi_neurons) # input size for bigram\n",
    "print(tri_neurons) # input size for trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# SET UNI-GRAM, BI-GRAM, TRI-GRAM IN THIS PART\n",
    "###############################################################################\n",
    "\n",
    "n_grams = 1 # UNI-GRAM\n",
    "# n_grams = 2 # BI-GRAM\n",
    "# n_grams = 3 # TRI-GRAM\n",
    "\n",
    "if n_grams == 1:\n",
    "    input_training_ANN = input_unigram_training \n",
    "    input_testing_ANN = input_unigram_testing\n",
    "    output_training_ANN = output_unigram_training\n",
    "    output_testing_ANN = output_unigram_testing\n",
    "    input_neurons = uni_neurons\n",
    "    print(\"Unigram Selected \")\n",
    "    \n",
    "elif n_grams == 2:\n",
    "    input_training_ANN = input_bigram_training\n",
    "    input_testing_ANN = input_bigram_testing\n",
    "    output_training_ANN = output_bigram_training\n",
    "    output_testing_ANN = output_bigram_testing\n",
    "    input_neurons = bi_neurons\n",
    "    print(\"Bigram Selected \")\n",
    "    \n",
    "elif n_grams == 3:\n",
    "    input_training_ANN = input_trigram_training\n",
    "    input_testing_ANN = input_trigram_testing\n",
    "    output_training_ANN = output_trigram_training\n",
    "    output_testing_ANN = output_trigram_testing\n",
    "    input_neurons = tri_neurons\n",
    "    print(\"Trigram Selected \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import tensorflow as tf \n",
    "#import tensorflow_text as text \n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.python.ops.init_ops import glorot_normal_initializer\n",
    "from tensorflow.python.ops.init_ops import glorot_uniform_initializer\n",
    "from tensorflow.python.ops.init_ops import he_normal\n",
    "from tensorflow.python.ops.init_ops import he_uniform\n",
    "from tensorflow.python.ops.init_ops import truncated_normal_initializer\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import array \n",
    "import pandas as pd\n",
    "\n",
    "from scipy.sparse import coo_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=123)\n",
    "# BEGIN CODE TENSORFLOW \n",
    "seed_id_number = 123\n",
    "tf.random.set_seed(seed_id_number)\n",
    "def build_model():\n",
    "    model = keras.Sequential([    \n",
    "    layers.Dense(500, activation='relu' ,\n",
    "                 input_shape=[input_neurons],\n",
    "#                  kernel_initializer = truncated_normal_initializer(mean=0.0, stddev=0.05, seed=seed_id_number),\n",
    "                 kernel_initializer = he_normal(seed=seed_id_number),\n",
    "                 bias_initializer = he_normal(seed=seed_id_number)\n",
    "                ), #1st hidden layer\n",
    "    layers.Dense(350, activation='relu',\n",
    "                 kernel_initializer = he_normal(seed=seed_id_number),\n",
    "                 bias_initializer = he_normal(seed=seed_id_number)\n",
    "                ), #2nd hidden layer\n",
    "    layers.Dense(200, activation='relu',\n",
    "                 kernel_initializer = he_normal(seed=seed_id_number),\n",
    "                 bias_initializer = he_normal(seed=seed_id_number)\n",
    "                ), #3rd hidden layer\n",
    "    layers.Dense(1, activation ='sigmoid',\n",
    "                kernel_initializer = glorot_normal_initializer(seed=seed_id_number),\n",
    "                bias_initializer = glorot_normal_initializer(seed=seed_id_number)\n",
    "                ), #output layer\n",
    "    layers.Dropout(0.5)\n",
    "    ])\n",
    "\n",
    "  # (Learning Rate, Momentum,nesterov)\n",
    "    optimizer = tf.keras.optimizers.SGD(0.01,0.9,False) \n",
    "\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mean_absolute_error','accuracy'])\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create ANN Model\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display training progress by printing a single dot for each completed epoch\n",
    "class PrintDot(keras.callbacks.Callback): #Early Stopping\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if epoch % 100 == 0: print('')\n",
    "        print('.', end='')\n",
    "##Set training Iterations\n",
    "# EPOCHS = 10\n",
    "EPOCHS = 200\n",
    "#Run Training Model in each unigram, bigram, trigram\n",
    "history = model.fit(\n",
    "    input_training_ANN, output_training_ANN, #80%\n",
    "    epochs=EPOCHS, \n",
    "    verbose=0, #0 or 1. Verbosity mode. 0 = silent, 1 = progress bar.\n",
    "    validation_split = 0.2, validation_steps=20,\n",
    "    callbacks=[PrintDot()])\n",
    "\n",
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "hist.tail() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print Weight and Bias for each layers\n",
    "weight_1, bias_1 = model.layers[0].get_weights()\n",
    "weight_2, bias_2 = model.layers[1].get_weights()\n",
    "weight_3, bias_3 = model.layers[2].get_weights()\n",
    "weight_4, bias_4 = model.layers[3].get_weights()\n",
    "\n",
    "print('1st layer weight :\\n',weight_1)\n",
    "print('1st layer bias : \\n',bias_1)\n",
    "\n",
    "print('2nd layer weight : \\n',weight_2)\n",
    "print('2nd layer bias : \\n',bias_2)\n",
    "\n",
    "print('3rd layer weight : \\n',weight_3)\n",
    "print('3rd layer bias : \\n',bias_3)\n",
    "\n",
    "print('4rd layer weight : \\n',weight_4)\n",
    "print('4rd layer bias : \\n',bias_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Plot 'Module'\n",
    "def plot_history(history):\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "    \n",
    "    ## Plot Target Output Graph\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle(' [TARGET OUTPUT]')\n",
    "    plt.plot(output_testing_ANN)\n",
    "    \n",
    "    ##Plot Mean Absolute Error vs Epoch Graph (train & validation)\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Abs Error [MPG]')\n",
    "    plt.plot(hist['epoch'], hist['mean_absolute_error'],\n",
    "           label='Train Error')\n",
    "    plt.plot(hist['epoch'], hist['val_mean_absolute_error'],\n",
    "           label = 'Validation Error')\n",
    "    plt.legend()\n",
    "    \n",
    "    ##Plot Mean Square Error vs Epoch graph (train & validation)\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Square Error [$MPG^2$]')\n",
    "    plt.plot(hist['epoch'], hist['loss'],\n",
    "           label='Train Error')\n",
    "    plt.plot(hist['epoch'], hist['val_loss'],\n",
    "           label = 'Validation Error')\n",
    "    plt.legend()\n",
    "    \n",
    "    ##Plot Validation (Train & validation)\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.plot(hist['epoch'], hist['accuracy'],\n",
    "            label='Train Accuracy')\n",
    "    plt.plot(hist['epoch'], hist['val_accuracy'],\n",
    "            label='Validation Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "##Calling plot 'Module'   \n",
    "plot_history(history)\n",
    "\n",
    "##########################\n",
    "#Run Testing Model \n",
    "##########################\n",
    "test_predictions = model.predict(input_testing_ANN)\n",
    "\n",
    "print(len(test_predictions))\n",
    "test_predictions1 = test_predictions[1]\n",
    "print(test_predictions1)\n",
    "\n",
    "##Plot Prediction Output Graph\n",
    "test_predictions[test_predictions >= 0.5] = 1\n",
    "test_predictions[test_predictions < 0.5] = 0\n",
    "\n",
    "##############################\n",
    "#Print Output Unigram Testing\n",
    "##############################\n",
    "print(output_testing_ANN)    #80\n",
    "print(test_predictions)\n",
    "\n",
    "plt.figure()\n",
    "plt.suptitle('Output Prediction')\n",
    "plt.plot(test_predictions)\n",
    "\n",
    "print(('Test Predictions = {}').format(test_predictions))\n",
    "\n",
    "test_predictions[test_predictions >= 0.5] = 1\n",
    "test_predictions[test_predictions < 0.5] = 0\n",
    "print(test_predictions)\n",
    "\n",
    "##Plot Unigram Prediction Error [MPG] graph \n",
    "error = test_predictions - output_testing_ANN\n",
    "plt.figure()\n",
    "plt.hist(error, bins = 25, label = \"Binary1\")\n",
    "plt.xlabel(\"Prediction Error [MPG]\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(test_predictions))\n",
    "print(type(input_testing_ANN)) \n",
    "print(type(df[40516:50645]))\n",
    "# df_testing = df_unigram.iloc[40516:50645,:].copy()\n",
    "df_testing = df_unigram.iloc[row_train_end:row_test_end,:].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing2 =  pd.DataFrame(test_predictions[:,0] , columns =['Test Predictions Binary1'])\n",
    "############################################################################\n",
    "## USE THIS FOR OUTPUT with 2 COLUMNS\n",
    "# df_testing2['Test Predictions Binary2'] =  pd.DataFrame(test_predictions[:,1])\n",
    "############################################################################\n",
    "df_testing2['Test Predictions Binary2'] =  pd.DataFrame(test_predictions[:,0])\n",
    "df_testing2['Text5'] = df_testing['Text6'].to_numpy()\n",
    "print(df_testing2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing3 = df_testing2[df_testing2['Test Predictions Binary1']==0] # Filter Non-Hoax Predictions\n",
    "# df_testing3 = df_testing2[df_testing2['Test Predictions Binary2']==1] # Filter Hoax Prediction\n",
    "print(df_testing3.head())\n",
    "print(df_testing3.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(df_testing3['Text5'])\n",
    "words = np.array(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()   \n",
    "cv_fit=cv.fit_transform(words) \n",
    "word_list = cv.get_feature_names();    \n",
    "count_list = cv_fit.toarray().sum(axis=0)    \n",
    "\n",
    "words_hoax = dict(zip(word_list,count_list))\n",
    "# print(words_hoax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_words_hoax = pd.DataFrame([words_hoax])\n",
    "df_words_hoax = pd.DataFrame(words_hoax.items(), columns=['Hoax Words', 'Frequency'])\n",
    "print(df_words_hoax.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words_hoax.to_csv('words_hoax.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing['Test Predictions Binary2'] = df_testing2['Test Predictions Binary2'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_testing.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing.to_csv('result_prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion Matrix\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "matrix = confusion_matrix(test_predictions.argmax(axis=1),output_testing_ANN.argmax(axis=1))\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_grams == 1:\n",
    "    vectors_testing_ANN = vectors_unigram[row_train_end:row_test_end]\n",
    "    tfidf_ANN = tfidf_unigram\n",
    "elif n_grams == 2:\n",
    "    vectors_testing_ANN = vectors_bigram[row_train_end:row_test_end]\n",
    "    tfidf_ANN = tfidf_bigram\n",
    "elif n_grams == 3:\n",
    "    vectors_testing_ANN = vectors_trigram[row_train_end:row_test_end]\n",
    "    tfidf_ANN = tfidf_trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2score = chi2(vectors_testing_ANN, test_predictions)[0] # 1 dimension output\n",
    "# chi2score = chi2(vectors_testing_ANN, test_predictions[:,1])[0] # 2 dimension output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tfidf_ANN.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "wscores = list(zip(tfidf_ANN.get_feature_names(), chi2score)) #list\n",
    "wscores1 = [row for row in wscores if not np.isnan(row[1])]  # remove NAN value\n",
    "wchi2 = sorted(wscores, key=lambda x:x[1]) #with NAN values\n",
    "# wchi2 = sorted(wscores1, key=lambda x:x[1]) #without NAN values\n",
    "topchi2 = list(zip(*wchi2[-30:]))\n",
    "print(topchi2)\n",
    "x = range(len(topchi2[1]))\n",
    "labels = topchi2[0]\n",
    "plt.barh(x,topchi2[1], align='center', alpha=0.2)\n",
    "plt.plot(topchi2[1], x, '-o', markersize=5, alpha=0.8)\n",
    "plt.yticks(x, labels)\n",
    "plt.xlabel('$\\chi^2$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def plot_cm(labels, predictions, p = 0.5):\n",
    "  cm = confusion_matrix(labels, predictions > p)\n",
    "  plt.figure(figsize=(5,5))\n",
    "  sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "  plt.title('Confusion matrix @{:.2f}'.format(p))\n",
    "  plt.ylabel('Actual label')\n",
    "  plt.xlabel('Predicted label')\n",
    "\n",
    "  print('Legitimate Transactions Detected (True Negatives): ', cm[0][0])\n",
    "  print('Legitimate Transactions Incorrectly Detected (False Positives): ', cm[0][1])\n",
    "  print('Fraudulent Transactions Missed (False Negatives): ', cm[1][0])\n",
    "  print('Fraudulent Transactions Detected (True Positives): ', cm[1][1])\n",
    "  print('Total Fraudulent Transactions: ', np.sum(cm[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_predictions)\n",
    "plot_cm(output_testing_ANN[:,0], test_predictions[:,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
